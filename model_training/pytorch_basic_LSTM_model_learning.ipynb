{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26ce6c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (0.11.1)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from torchtext) (1.19.5)\n",
      "Requirement already satisfied: torch==1.10.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from torchtext) (1.10.1)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from torchtext) (4.62.3)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from torchtext) (2.25.1)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from torch==1.10.1->torchtext) (3.10.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->torchtext) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->torchtext) (1.26.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->torchtext) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->torchtext) (2021.5.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install torch\n",
    "%pip install torchtext\n",
    "# import torch\n",
    "# from torch import nn\n",
    "# from torchtext.data.utils import get_tokenizer\n",
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "# from torchtext.legacy.data import Field, TabularDataset, BucketIterator\n",
    "\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "995e5646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a7faeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic text cleanup\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_url(text): \n",
    "    url_pattern  = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    return url_pattern.sub(r'', text)\n",
    " # converting return value from list to string\n",
    "\n",
    "def clean_text(text): \n",
    "    delete_dict = {sp_character: '' for sp_character in string.punctuation} \n",
    "    delete_dict[' '] = ' ' \n",
    "    table = str.maketrans(delete_dict)\n",
    "    text1 = text.translate(table)\n",
    "    #print('cleaned:'+text1)\n",
    "    textArr= text1.split()\n",
    "    text2 = ' '.join([w for w in textArr if ( not w.isdigit() and  ( not w.isdigit() and len(w)>2))]) \n",
    "    return text2.lower()\n",
    "\n",
    "\n",
    "def remove_stopwords(message_text):\n",
    "    \"\"\"Remove any stopwords that bear no meaning\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    try:\n",
    "        word_tokens = word_tokenize(message_text)\n",
    "        filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "        filtered_sentence = \" \".join(filtered_sentence)\n",
    "        return filtered_sentence\n",
    "    except IndexError:\n",
    "        return message_text\n",
    "        \n",
    "\n",
    "\n",
    "def remove_reply_lines(message_text):\n",
    "    \"\"\"Try and remove as many reply lines as we can\"\"\"\n",
    "    message_text = \"\".join(line for line in message_text if not line.startswith('>'))\n",
    "    return message_text\n",
    "\n",
    "\n",
    "def remove_non_english(message_text):\n",
    "    \"\"\"Remove anything remotely not English. If it is not in English, it should not be processed in ANY form\"\"\"\n",
    "    words = set(nltk.corpus.words.words())\n",
    "    filtered_sentence = \" \".join(w for w in nltk.wordpunct_tokenize(message_text) \\\n",
    "                                 if w.lower() in words or not w.isalpha())\n",
    "    return filtered_sentence\n",
    "\n",
    "\n",
    "def remove_emails(message_text):\n",
    "    \"\"\"Finally, remove any links, emails and other non-essential trash from the text\"\"\"\n",
    "    text = re.sub(r\"\\S*@\\S*\\s?\", '', message_text, flags=re.MULTILINE)\n",
    "    text = re.sub(r\"<.*?>\", '', text, flags=re.MULTILINE)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83a9f7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Train data--------\n",
      "3     800\n",
      "7     800\n",
      "1     799\n",
      "5     799\n",
      "6     719\n",
      "8     582\n",
      "11    482\n",
      "2     362\n",
      "9     350\n",
      "4     278\n",
      "10    268\n",
      "Name: tag, dtype: int64\n",
      "6239\n",
      "-------------------------\n",
      "-------Test data--------\n",
      "3     200\n",
      "5     200\n",
      "7     200\n",
      "1     199\n",
      "6     180\n",
      "8     146\n",
      "11    120\n",
      "2      91\n",
      "9      88\n",
      "4      69\n",
      "10     68\n",
      "Name: tag, dtype: int64\n",
      "1561\n",
      "-------------------------\n",
      "Train Max Message Length :33468\n",
      "Test Max Message Length :10910\n"
     ]
    }
   ],
   "source": [
    "train_data= pd.read_csv(\"/Users/yaroslavkizyma/Desktop/json_data_exports_alternative/encoded_data_alternative_train.csv\")\n",
    "train_data.dropna(axis = 0, how ='any', inplace=True) \n",
    "train_data['Num_words_text'] = train_data['message'].apply(lambda x:len(str(x).split())) \n",
    "mask = train_data['Num_words_text'] > 0\n",
    "train_data = train_data[mask]\n",
    "print('-------Train data--------')\n",
    "print(train_data['tag'].value_counts())\n",
    "print(len(train_data))\n",
    "print('-------------------------')\n",
    "max_train_sentence_length  = train_data['Num_words_text'].max()\n",
    "\n",
    "\n",
    "train_data['message'] = train_data['message'].apply(remove_emoji)\n",
    "train_data['message'] = train_data['message'].apply(remove_stopwords)\n",
    "# train_data['message'] = train_data['message'].apply(remove_reply_lines)\n",
    "train_data['message'] = train_data['message'].apply(remove_non_english)\n",
    "train_data['message'] = train_data['message'].apply(remove_emails)\n",
    "train_data['message'] = train_data['message'].apply(remove_url)\n",
    "train_data['message'] = train_data['message'].apply(clean_text)\n",
    "\n",
    "\n",
    "test_data= pd.read_csv(\"/Users/yaroslavkizyma/Desktop/json_data_exports_alternative/encoded_data_alternative_test.csv\")\n",
    "test_data.dropna(axis = 0, how ='any', inplace=True) \n",
    "test_data['Num_words_text'] = test_data['message'].apply(lambda x:len(str(x).split())) \n",
    "\n",
    "max_test_sentence_length  = test_data['Num_words_text'].max()\n",
    "\n",
    "mask = test_data['Num_words_text'] > 0\n",
    "test_data = test_data[mask]\n",
    "\n",
    "print('-------Test data--------')\n",
    "print(test_data['tag'].value_counts())\n",
    "print(len(test_data))\n",
    "print('-------------------------')\n",
    "\n",
    "test_data['message'] = test_data['message'].apply(remove_emoji)\n",
    "test_data['message'] = test_data['message'].apply(remove_stopwords)\n",
    "# test_data['message'] = test_data['message'].apply(remove_reply_lines)\n",
    "test_data['message'] = test_data['message'].apply(remove_non_english)\n",
    "test_data['message'] = test_data['message'].apply(remove_emails)\n",
    "test_data['message'] = test_data['message'].apply(remove_url)\n",
    "test_data['message'] = test_data['message'].apply(clean_text)\n",
    "\n",
    "print('Train Max Message Length :'+str(max_train_sentence_length))\n",
    "print('Test Max Message Length :'+str(max_test_sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df0b8a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tag</th>\n",
       "      <th>Num_words_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unfortunately link link kind file program reca...</td>\n",
       "      <td>8</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neo order price need conformation address link...</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dear sir hit export button nothing seen progre...</td>\n",
       "      <td>11</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dear finally save attached let know need anyth...</td>\n",
       "      <td>5</td>\n",
       "      <td>438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>name description hello expiration date members...</td>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>said partial refund would like full refund ple...</td>\n",
       "      <td>7</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hello hope well thank know issue ending please...</td>\n",
       "      <td>9</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hello thank like refund please support thanks ...</td>\n",
       "      <td>7</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>message help sure mean working recently tried ...</td>\n",
       "      <td>5</td>\n",
       "      <td>437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>good morning understand entirely without quest...</td>\n",
       "      <td>5</td>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tag  Num_words_text\n",
       "0  unfortunately link link kind file program reca...    8             116\n",
       "1  neo order price need conformation address link...    8              45\n",
       "2  dear sir hit export button nothing seen progre...   11             148\n",
       "3  dear finally save attached let know need anyth...    5             438\n",
       "4  name description hello expiration date members...    4              38\n",
       "5  said partial refund would like full refund ple...    7              97\n",
       "6  hello hope well thank know issue ending please...    9              81\n",
       "7  hello thank like refund please support thanks ...    7             176\n",
       "8  message help sure mean working recently tried ...    5             437\n",
       "9  good morning understand entirely without quest...    5             313"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fde7568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data len:4991\n",
      "Class distribution Counter({7: 640, 3: 640, 1: 639, 5: 639, 6: 575, 8: 466, 11: 386, 2: 290, 9: 280, 4: 222, 10: 214})\n",
      "Valid data len:1248\n",
      "Class distribution Counter({3: 160, 5: 160, 7: 160, 1: 160, 6: 144, 8: 116, 11: 96, 2: 72, 9: 70, 4: 56, 10: 54})\n",
      "Test data len:1561\n",
      "Class distribution Counter({3: 200, 5: 200, 7: 200, 1: 199, 6: 180, 8: 146, 11: 120, 2: 91, 9: 88, 4: 69, 10: 68})\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(train_data['message'].tolist(),\\\n",
    "                                                      train_data['tag'].tolist(),\\\n",
    "                                                      test_size=0.2,\\\n",
    "                                                      stratify = train_data['tag'].tolist(),\\\n",
    "                                                      random_state=0)\n",
    "\n",
    "\n",
    "print('Train data len:'+str(len(X_train)))\n",
    "print('Class distribution '+str(Counter(Y_train)))\n",
    "\n",
    "\n",
    "print('Valid data len:'+str(len(X_valid)))\n",
    "print('Class distribution '+ str(Counter(Y_valid)))\n",
    "\n",
    "print('Test data len:'+str(len(test_data['message'].tolist())))\n",
    "print('Class distribution '+ str(Counter(test_data['tag'].tolist())))\n",
    "\n",
    "\n",
    "train_dat =list(zip(Y_train,X_train))\n",
    "valid_dat =list(zip(Y_valid,X_valid))\n",
    "test_dat=list(zip(test_data['tag'].tolist(),test_data['message'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "308d8fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty elem removed\n",
      "Empty elem removed\n",
      "Empty elem removed\n",
      "Empty elem removed\n",
      "Empty elem removed\n",
      "Empty elem removed\n",
      "Empty elem removed\n",
      "Empty elem removed\n",
      "Empty elem removed\n",
      "Empty elem removed\n",
      "Empty elem removed\n",
      "Empty elem removed\n",
      "Empty elem removed\n"
     ]
    }
   ],
   "source": [
    "# redo into a function proper\n",
    "\n",
    "for tuple_elem in train_dat:\n",
    "    class_tag, text = tuple_elem\n",
    "    if len(text) == 0:\n",
    "        train_dat.remove(tuple_elem)\n",
    "        print(f\"Empty elem removed\")\n",
    "\n",
    "for tuple_elem in test_dat:\n",
    "    class_tag, text = tuple_elem\n",
    "    if len(text) == 0:\n",
    "        test_dat.remove(tuple_elem)\n",
    "        print(f\"Empty elem removed\")\n",
    "\n",
    "\n",
    "for tuple_elem in valid_dat:\n",
    "    class_tag, text = tuple_elem\n",
    "    if len(text) == 0:\n",
    "        valid_dat.remove(tuple_elem)\n",
    "        print(f\"Empty elem removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62b7030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7aa5dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = train_dat\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c73fd4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcaedfd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[309, 0, 1084, 0, 425]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline('here is the an example')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ab35df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pipeline('1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2bbad23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "         label_list.append(label_pipeline(_label))\n",
    "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "         text_list.append(processed_text)\n",
    "         offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "#train_iter = train_dat\n",
    "#dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a499eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTM_variable_input(torch.nn.Module) :\n",
    "#     def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "#         super().__init__()\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.dropout = nn.Dropout(0.3)\n",
    "#         self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "#         self.linear = nn.Linear(hidden_dim, 5)\n",
    "        \n",
    "#     def forward(self, x, s):\n",
    "#         x = self.embeddings(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x_pack = pack_padded_sequence(x, s, batch_first=True, enforce_sorted=False)\n",
    "#         out_pack, (ht, ct) = self.lstm(x_pack)\n",
    "#         out = self.linear(ht[-1])\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae98250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_fixed_len(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_class) :\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=3)\n",
    "        self.linear = nn.Linear(hidden_dim, num_class)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, l):\n",
    "        len_ = len(x)\n",
    "        if len_ < 1:\n",
    "            print(len_)\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x.view(len_, 1, -1))\n",
    "        \n",
    "        return self.linear(lstm_out[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8f866e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "7941\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "train_iter1 = train_dat\n",
    "num_class = len(set([label for (label, text) in train_iter1]))\n",
    "print(num_class)\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)\n",
    "# embedding_size\n",
    "emsize = 16\n",
    "# hidden_dim\n",
    "h_dim = 16\n",
    "# model = LSTM_variable_input(vocab_size, emsize, num_class)\n",
    "model =  LSTM_fixed_len(vocab_size, emsize, h_dim, num_class)\n",
    "\n",
    "# model = TextClassificationModel(vocab_size, emsize, num_class).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c9b50621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predited_label = model(text, offsets)\n",
    "        loss = criterion(predited_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predited_label = model(text, offsets)\n",
    "            loss = criterion(predited_label, label)\n",
    "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "317f21e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 4982 batches | accuracy    0.140\n",
      "| epoch   1 |  1000/ 4982 batches | accuracy    0.152\n",
      "| epoch   1 |  1500/ 4982 batches | accuracy    0.160\n",
      "| epoch   1 |  2000/ 4982 batches | accuracy    0.156\n",
      "| epoch   1 |  2500/ 4982 batches | accuracy    0.154\n",
      "| epoch   1 |  3000/ 4982 batches | accuracy    0.174\n",
      "| epoch   1 |  3500/ 4982 batches | accuracy    0.186\n",
      "| epoch   1 |  4000/ 4982 batches | accuracy    0.188\n",
      "| epoch   1 |  4500/ 4982 batches | accuracy    0.192\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 98.34s | valid accuracy    0.219 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 4982 batches | accuracy    0.204\n",
      "| epoch   2 |  1000/ 4982 batches | accuracy    0.220\n",
      "| epoch   2 |  1500/ 4982 batches | accuracy    0.248\n",
      "| epoch   2 |  2000/ 4982 batches | accuracy    0.230\n",
      "| epoch   2 |  2500/ 4982 batches | accuracy    0.234\n",
      "| epoch   2 |  3000/ 4982 batches | accuracy    0.226\n",
      "| epoch   2 |  3500/ 4982 batches | accuracy    0.242\n",
      "| epoch   2 |  4000/ 4982 batches | accuracy    0.210\n",
      "| epoch   2 |  4500/ 4982 batches | accuracy    0.268\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 97.10s | valid accuracy    0.262 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 4982 batches | accuracy    0.232\n",
      "| epoch   3 |  1000/ 4982 batches | accuracy    0.234\n",
      "| epoch   3 |  1500/ 4982 batches | accuracy    0.246\n",
      "| epoch   3 |  2000/ 4982 batches | accuracy    0.230\n",
      "| epoch   3 |  2500/ 4982 batches | accuracy    0.298\n",
      "| epoch   3 |  3000/ 4982 batches | accuracy    0.292\n",
      "| epoch   3 |  3500/ 4982 batches | accuracy    0.260\n",
      "| epoch   3 |  4000/ 4982 batches | accuracy    0.296\n",
      "| epoch   3 |  4500/ 4982 batches | accuracy    0.268\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 101.45s | valid accuracy    0.284 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 4982 batches | accuracy    0.281\n",
      "| epoch   4 |  1000/ 4982 batches | accuracy    0.278\n",
      "| epoch   4 |  1500/ 4982 batches | accuracy    0.302\n",
      "| epoch   4 |  2000/ 4982 batches | accuracy    0.306\n",
      "| epoch   4 |  2500/ 4982 batches | accuracy    0.306\n",
      "| epoch   4 |  3000/ 4982 batches | accuracy    0.306\n",
      "| epoch   4 |  3500/ 4982 batches | accuracy    0.290\n",
      "| epoch   4 |  4000/ 4982 batches | accuracy    0.262\n",
      "| epoch   4 |  4500/ 4982 batches | accuracy    0.312\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 98.06s | valid accuracy    0.302 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 4982 batches | accuracy    0.325\n",
      "| epoch   5 |  1000/ 4982 batches | accuracy    0.274\n",
      "| epoch   5 |  1500/ 4982 batches | accuracy    0.296\n",
      "| epoch   5 |  2000/ 4982 batches | accuracy    0.308\n",
      "| epoch   5 |  2500/ 4982 batches | accuracy    0.328\n",
      "| epoch   5 |  3000/ 4982 batches | accuracy    0.332\n",
      "| epoch   5 |  3500/ 4982 batches | accuracy    0.336\n",
      "| epoch   5 |  4000/ 4982 batches | accuracy    0.322\n",
      "| epoch   5 |  4500/ 4982 batches | accuracy    0.364\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 97.80s | valid accuracy    0.321 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 4982 batches | accuracy    0.333\n",
      "| epoch   6 |  1000/ 4982 batches | accuracy    0.330\n",
      "| epoch   6 |  1500/ 4982 batches | accuracy    0.350\n",
      "| epoch   6 |  2000/ 4982 batches | accuracy    0.332\n",
      "| epoch   6 |  2500/ 4982 batches | accuracy    0.360\n",
      "| epoch   6 |  3000/ 4982 batches | accuracy    0.332\n",
      "| epoch   6 |  3500/ 4982 batches | accuracy    0.352\n",
      "| epoch   6 |  4000/ 4982 batches | accuracy    0.366\n",
      "| epoch   6 |  4500/ 4982 batches | accuracy    0.364\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 97.88s | valid accuracy    0.362 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 4982 batches | accuracy    0.379\n",
      "| epoch   7 |  1000/ 4982 batches | accuracy    0.396\n",
      "| epoch   7 |  1500/ 4982 batches | accuracy    0.380\n",
      "| epoch   7 |  2000/ 4982 batches | accuracy    0.324\n",
      "| epoch   7 |  2500/ 4982 batches | accuracy    0.346\n",
      "| epoch   7 |  3000/ 4982 batches | accuracy    0.414\n",
      "| epoch   7 |  3500/ 4982 batches | accuracy    0.394\n",
      "| epoch   7 |  4000/ 4982 batches | accuracy    0.344\n",
      "| epoch   7 |  4500/ 4982 batches | accuracy    0.366\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 98.67s | valid accuracy    0.366 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 4982 batches | accuracy    0.361\n",
      "| epoch   8 |  1000/ 4982 batches | accuracy    0.390\n",
      "| epoch   8 |  1500/ 4982 batches | accuracy    0.346\n",
      "| epoch   8 |  2000/ 4982 batches | accuracy    0.412\n",
      "| epoch   8 |  2500/ 4982 batches | accuracy    0.362\n",
      "| epoch   8 |  3000/ 4982 batches | accuracy    0.336\n",
      "| epoch   8 |  3500/ 4982 batches | accuracy    0.386\n",
      "| epoch   8 |  4000/ 4982 batches | accuracy    0.376\n",
      "| epoch   8 |  4500/ 4982 batches | accuracy    0.370\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 98.20s | valid accuracy    0.368 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 4982 batches | accuracy    0.401\n",
      "| epoch   9 |  1000/ 4982 batches | accuracy    0.420\n",
      "| epoch   9 |  1500/ 4982 batches | accuracy    0.368\n",
      "| epoch   9 |  2000/ 4982 batches | accuracy    0.394\n",
      "| epoch   9 |  2500/ 4982 batches | accuracy    0.416\n",
      "| epoch   9 |  3000/ 4982 batches | accuracy    0.412\n",
      "| epoch   9 |  3500/ 4982 batches | accuracy    0.378\n",
      "| epoch   9 |  4000/ 4982 batches | accuracy    0.420\n",
      "| epoch   9 |  4500/ 4982 batches | accuracy    0.380\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 97.60s | valid accuracy    0.384 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 4982 batches | accuracy    0.401\n",
      "| epoch  10 |  1000/ 4982 batches | accuracy    0.430\n",
      "| epoch  10 |  1500/ 4982 batches | accuracy    0.432\n",
      "| epoch  10 |  2000/ 4982 batches | accuracy    0.394\n",
      "| epoch  10 |  2500/ 4982 batches | accuracy    0.372\n",
      "| epoch  10 |  3000/ 4982 batches | accuracy    0.426\n",
      "| epoch  10 |  3500/ 4982 batches | accuracy    0.406\n",
      "| epoch  10 |  4000/ 4982 batches | accuracy    0.434\n",
      "| epoch  10 |  4500/ 4982 batches | accuracy    0.418\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 98.14s | valid accuracy    0.379 \n",
      "-----------------------------------------------------------\n",
      "| epoch  11 |   500/ 4982 batches | accuracy    0.377\n",
      "| epoch  11 |  1000/ 4982 batches | accuracy    0.436\n",
      "| epoch  11 |  1500/ 4982 batches | accuracy    0.422\n",
      "| epoch  11 |  2000/ 4982 batches | accuracy    0.422\n",
      "| epoch  11 |  2500/ 4982 batches | accuracy    0.410\n",
      "| epoch  11 |  3000/ 4982 batches | accuracy    0.452\n",
      "| epoch  11 |  3500/ 4982 batches | accuracy    0.424\n",
      "| epoch  11 |  4000/ 4982 batches | accuracy    0.422\n",
      "| epoch  11 |  4500/ 4982 batches | accuracy    0.484\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  11 | time: 98.08s | valid accuracy    0.407 \n",
      "-----------------------------------------------------------\n",
      "| epoch  12 |   500/ 4982 batches | accuracy    0.445\n",
      "| epoch  12 |  1000/ 4982 batches | accuracy    0.454\n",
      "| epoch  12 |  1500/ 4982 batches | accuracy    0.428\n",
      "| epoch  12 |  2000/ 4982 batches | accuracy    0.474\n",
      "| epoch  12 |  2500/ 4982 batches | accuracy    0.448\n",
      "| epoch  12 |  3000/ 4982 batches | accuracy    0.448\n",
      "| epoch  12 |  3500/ 4982 batches | accuracy    0.404\n",
      "| epoch  12 |  4000/ 4982 batches | accuracy    0.436\n",
      "| epoch  12 |  4500/ 4982 batches | accuracy    0.458\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  12 | time: 98.00s | valid accuracy    0.405 \n",
      "-----------------------------------------------------------\n",
      "| epoch  13 |   500/ 4982 batches | accuracy    0.447\n",
      "| epoch  13 |  1000/ 4982 batches | accuracy    0.398\n",
      "| epoch  13 |  1500/ 4982 batches | accuracy    0.426\n",
      "| epoch  13 |  2000/ 4982 batches | accuracy    0.492\n",
      "| epoch  13 |  2500/ 4982 batches | accuracy    0.466\n",
      "| epoch  13 |  3000/ 4982 batches | accuracy    0.452\n",
      "| epoch  13 |  3500/ 4982 batches | accuracy    0.438\n",
      "| epoch  13 |  4000/ 4982 batches | accuracy    0.440\n",
      "| epoch  13 |  4500/ 4982 batches | accuracy    0.456\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  13 | time: 98.10s | valid accuracy    0.406 \n",
      "-----------------------------------------------------------\n",
      "| epoch  14 |   500/ 4982 batches | accuracy    0.447\n",
      "| epoch  14 |  1000/ 4982 batches | accuracy    0.446\n",
      "| epoch  14 |  1500/ 4982 batches | accuracy    0.466\n",
      "| epoch  14 |  2000/ 4982 batches | accuracy    0.448\n",
      "| epoch  14 |  2500/ 4982 batches | accuracy    0.414\n",
      "| epoch  14 |  3000/ 4982 batches | accuracy    0.444\n",
      "| epoch  14 |  3500/ 4982 batches | accuracy    0.454\n",
      "| epoch  14 |  4000/ 4982 batches | accuracy    0.494\n",
      "| epoch  14 |  4500/ 4982 batches | accuracy    0.474\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  14 | time: 101.67s | valid accuracy    0.404 \n",
      "-----------------------------------------------------------\n",
      "| epoch  15 |   500/ 4982 batches | accuracy    0.455\n",
      "| epoch  15 |  1000/ 4982 batches | accuracy    0.486\n",
      "| epoch  15 |  1500/ 4982 batches | accuracy    0.450\n",
      "| epoch  15 |  2000/ 4982 batches | accuracy    0.434\n",
      "| epoch  15 |  2500/ 4982 batches | accuracy    0.460\n",
      "| epoch  15 |  3000/ 4982 batches | accuracy    0.472\n",
      "| epoch  15 |  3500/ 4982 batches | accuracy    0.460\n",
      "| epoch  15 |  4000/ 4982 batches | accuracy    0.448\n",
      "| epoch  15 |  4500/ 4982 batches | accuracy    0.450\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  15 | time: 102.47s | valid accuracy    0.404 \n",
      "-----------------------------------------------------------\n",
      "| epoch  16 |   500/ 4982 batches | accuracy    0.441\n",
      "| epoch  16 |  1000/ 4982 batches | accuracy    0.466\n",
      "| epoch  16 |  1500/ 4982 batches | accuracy    0.484\n",
      "| epoch  16 |  2000/ 4982 batches | accuracy    0.430\n",
      "| epoch  16 |  2500/ 4982 batches | accuracy    0.430\n",
      "| epoch  16 |  3000/ 4982 batches | accuracy    0.458\n",
      "| epoch  16 |  3500/ 4982 batches | accuracy    0.426\n",
      "| epoch  16 |  4000/ 4982 batches | accuracy    0.450\n",
      "| epoch  16 |  4500/ 4982 batches | accuracy    0.472\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  16 | time: 102.29s | valid accuracy    0.404 \n",
      "-----------------------------------------------------------\n",
      "| epoch  17 |   500/ 4982 batches | accuracy    0.461\n",
      "| epoch  17 |  1000/ 4982 batches | accuracy    0.470\n",
      "| epoch  17 |  1500/ 4982 batches | accuracy    0.412\n",
      "| epoch  17 |  2000/ 4982 batches | accuracy    0.490\n",
      "| epoch  17 |  2500/ 4982 batches | accuracy    0.430\n",
      "| epoch  17 |  3000/ 4982 batches | accuracy    0.454\n",
      "| epoch  17 |  3500/ 4982 batches | accuracy    0.426\n",
      "| epoch  17 |  4000/ 4982 batches | accuracy    0.474\n",
      "| epoch  17 |  4500/ 4982 batches | accuracy    0.420\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  17 | time: 101.82s | valid accuracy    0.404 \n",
      "-----------------------------------------------------------\n",
      "| epoch  18 |   500/ 4982 batches | accuracy    0.439\n",
      "| epoch  18 |  1000/ 4982 batches | accuracy    0.470\n",
      "| epoch  18 |  1500/ 4982 batches | accuracy    0.458\n",
      "| epoch  18 |  2000/ 4982 batches | accuracy    0.460\n",
      "| epoch  18 |  2500/ 4982 batches | accuracy    0.412\n",
      "| epoch  18 |  3000/ 4982 batches | accuracy    0.462\n",
      "| epoch  18 |  3500/ 4982 batches | accuracy    0.466\n",
      "| epoch  18 |  4000/ 4982 batches | accuracy    0.454\n",
      "| epoch  18 |  4500/ 4982 batches | accuracy    0.418\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  18 | time: 101.85s | valid accuracy    0.404 \n",
      "-----------------------------------------------------------\n",
      "| epoch  19 |   500/ 4982 batches | accuracy    0.457\n",
      "| epoch  19 |  1000/ 4982 batches | accuracy    0.462\n",
      "| epoch  19 |  1500/ 4982 batches | accuracy    0.456\n",
      "| epoch  19 |  2000/ 4982 batches | accuracy    0.446\n",
      "| epoch  19 |  2500/ 4982 batches | accuracy    0.452\n",
      "| epoch  19 |  3000/ 4982 batches | accuracy    0.428\n",
      "| epoch  19 |  3500/ 4982 batches | accuracy    0.458\n",
      "| epoch  19 |  4000/ 4982 batches | accuracy    0.442\n",
      "| epoch  19 |  4500/ 4982 batches | accuracy    0.424\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  19 | time: 101.47s | valid accuracy    0.404 \n",
      "-----------------------------------------------------------\n",
      "| epoch  20 |   500/ 4982 batches | accuracy    0.465\n",
      "| epoch  20 |  1000/ 4982 batches | accuracy    0.444\n",
      "| epoch  20 |  1500/ 4982 batches | accuracy    0.458\n",
      "| epoch  20 |  2000/ 4982 batches | accuracy    0.454\n",
      "| epoch  20 |  2500/ 4982 batches | accuracy    0.470\n",
      "| epoch  20 |  3000/ 4982 batches | accuracy    0.448\n",
      "| epoch  20 |  3500/ 4982 batches | accuracy    0.416\n",
      "| epoch  20 |  4000/ 4982 batches | accuracy    0.446\n",
      "| epoch  20 |  4500/ 4982 batches | accuracy    0.448\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  20 | time: 101.00s | valid accuracy    0.404 \n",
      "-----------------------------------------------------------\n",
      "| epoch  21 |   500/ 4982 batches | accuracy    0.429\n",
      "| epoch  21 |  1000/ 4982 batches | accuracy    0.430\n",
      "| epoch  21 |  1500/ 4982 batches | accuracy    0.422\n",
      "| epoch  21 |  2000/ 4982 batches | accuracy    0.450\n",
      "| epoch  21 |  2500/ 4982 batches | accuracy    0.432\n",
      "| epoch  21 |  3000/ 4982 batches | accuracy    0.464\n",
      "| epoch  21 |  3500/ 4982 batches | accuracy    0.472\n",
      "| epoch  21 |  4000/ 4982 batches | accuracy    0.474\n",
      "| epoch  21 |  4500/ 4982 batches | accuracy    0.398\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  21 | time: 101.01s | valid accuracy    0.404 \n",
      "-----------------------------------------------------------\n",
      "| epoch  22 |   500/ 4982 batches | accuracy    0.471\n",
      "| epoch  22 |  1000/ 4982 batches | accuracy    0.466\n",
      "| epoch  22 |  1500/ 4982 batches | accuracy    0.462\n",
      "| epoch  22 |  2000/ 4982 batches | accuracy    0.438\n",
      "| epoch  22 |  2500/ 4982 batches | accuracy    0.434\n",
      "| epoch  22 |  3000/ 4982 batches | accuracy    0.468\n",
      "| epoch  22 |  3500/ 4982 batches | accuracy    0.428\n",
      "| epoch  22 |  4000/ 4982 batches | accuracy    0.490\n",
      "| epoch  22 |  4500/ 4982 batches | accuracy    0.458\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  22 | time: 101.14s | valid accuracy    0.404 \n",
      "-----------------------------------------------------------\n",
      "| epoch  23 |   500/ 4982 batches | accuracy    0.433\n",
      "| epoch  23 |  1000/ 4982 batches | accuracy    0.456\n",
      "| epoch  23 |  1500/ 4982 batches | accuracy    0.430\n",
      "| epoch  23 |  2000/ 4982 batches | accuracy    0.448\n",
      "| epoch  23 |  2500/ 4982 batches | accuracy    0.418\n",
      "| epoch  23 |  3000/ 4982 batches | accuracy    0.428\n",
      "| epoch  23 |  3500/ 4982 batches | accuracy    0.466\n",
      "| epoch  23 |  4000/ 4982 batches | accuracy    0.474\n",
      "| epoch  23 |  4500/ 4982 batches | accuracy    0.432\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  23 | time: 100.81s | valid accuracy    0.404 \n",
      "-----------------------------------------------------------\n",
      "| epoch  24 |   500/ 4982 batches | accuracy    0.459\n",
      "| epoch  24 |  1000/ 4982 batches | accuracy    0.418\n",
      "| epoch  24 |  1500/ 4982 batches | accuracy    0.432\n",
      "| epoch  24 |  2000/ 4982 batches | accuracy    0.444\n",
      "| epoch  24 |  2500/ 4982 batches | accuracy    0.416\n",
      "| epoch  24 |  3000/ 4982 batches | accuracy    0.454\n",
      "| epoch  24 |  3500/ 4982 batches | accuracy    0.458\n",
      "| epoch  24 |  4000/ 4982 batches | accuracy    0.490\n",
      "| epoch  24 |  4500/ 4982 batches | accuracy    0.426\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  24 | time: 101.11s | valid accuracy    0.404 \n",
      "-----------------------------------------------------------\n",
      "| epoch  25 |   500/ 4982 batches | accuracy    0.445\n",
      "| epoch  25 |  1000/ 4982 batches | accuracy    0.460\n",
      "| epoch  25 |  1500/ 4982 batches | accuracy    0.470\n",
      "| epoch  25 |  2000/ 4982 batches | accuracy    0.460\n",
      "| epoch  25 |  2500/ 4982 batches | accuracy    0.442\n",
      "| epoch  25 |  3000/ 4982 batches | accuracy    0.420\n",
      "| epoch  25 |  3500/ 4982 batches | accuracy    0.462\n",
      "| epoch  25 |  4000/ 4982 batches | accuracy    0.468\n",
      "| epoch  25 |  4500/ 4982 batches | accuracy    0.482\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  25 | time: 101.50s | valid accuracy    0.404 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "# Hyperparameters\n",
    "EPOCHS = 25 # epoch\n",
    "LR = 0.01 # learning rate\n",
    "# BATCH_SIZE = 16 # batch size for training\n",
    "BATCH_SIZE = 1 # batch size for training\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "\n",
    "train_iter2 = train_dat\n",
    "test_iter2 = test_dat \n",
    "valid_iter2= valid_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_iter2, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(valid_iter2, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_iter2, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "\n",
    "# for index, data in enumerate(train_dataloader):\n",
    "#     if len(data[1]) == 0:\n",
    "#         print('Alarm')\n",
    "#         print(index)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "      scheduler.step()\n",
    "    else:\n",
    "       total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78657c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155b9715",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-course",
   "language": "python",
   "name": "ml-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
